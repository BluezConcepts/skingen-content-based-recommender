{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Ingredient Data Cleaning\n",
    "## Paula's Choice Ingredient Dictionary Analysis\n",
    "\n",
    "This notebook cleans and explores the scraped ingredient data from Paula's Choice.\n",
    "\n",
    "### Objectives:\n",
    "1. Load both CSV files\n",
    "2. Explore data structure and quality\n",
    "3. Identify and remove duplicates\n",
    "4. Create a clean, unified dataset\n",
    "5. Perform exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Seaborn styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV files\n",
    "df1 = pd.read_csv('/mnt/user-data/uploads/paulas_choice_perfect_glance__1.csv')\n",
    "df2 = pd.read_csv('/mnt/user-data/uploads/paulas_choice_perfect_glance_2.csv')\n",
    "\n",
    "print(f\"File 1 shape: {df1.shape}\")\n",
    "print(f\"File 2 shape: {df2.shape}\")\n",
    "print(f\"\\nTotal rows before merge: {df1.shape[0] + df2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"Column names:\")\n",
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows from each file\n",
    "print(\"First 3 rows from File 1:\")\n",
    "display(df1.head(3))\n",
    "\n",
    "print(\"\\nFirst 3 rows from File 2:\")\n",
    "display(df2.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"File 1 Info:\")\n",
    "df1.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"File 2 Info:\")\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate ingredient names within each file\n",
    "# Note: We only check ingredient_name for duplicates, not rating/benefits/categories\n",
    "# (those columns will naturally have duplicates across different ingredients)\n",
    "print(\"Duplicate INGREDIENT NAMES in File 1:\")\n",
    "dup1 = df1['ingredient_name'].duplicated().sum()\n",
    "print(f\"  Count: {dup1}\")\n",
    "\n",
    "print(\"\\nDuplicate INGREDIENT NAMES in File 2:\")\n",
    "dup2 = df2['ingredient_name'].duplicated().sum()\n",
    "print(f\"  Count: {dup2}\")\n",
    "\n",
    "# Show some duplicate examples if they exist\n",
    "if dup1 > 0:\n",
    "    print(\"\\nDuplicate ingredient name examples in File 1:\")\n",
    "    dup_names = df1[df1['ingredient_name'].duplicated(keep=False)]['ingredient_name'].unique()[:5]\n",
    "    for name in dup_names:\n",
    "        print(f\"  - {name}\")\n",
    "        display(df1[df1['ingredient_name'] == name][['ingredient_name', 'rating', 'categories']])\n",
    "\n",
    "if dup2 > 0:\n",
    "    print(\"\\nDuplicate ingredient name examples in File 2:\")\n",
    "    dup_names = df2[df2['ingredient_name'].duplicated(keep=False)]['ingredient_name'].unique()[:5]\n",
    "    for name in dup_names:\n",
    "        print(f\"  - {name}\")\n",
    "        display(df2[df2['ingredient_name'] == name][['ingredient_name', 'rating', 'categories']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlap between files\n",
    "overlap = set(df1['ingredient_name']) & set(df2['ingredient_name'])\n",
    "print(f\"Number of overlapping ingredients between files: {len(overlap)}\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(\"\\nFirst 10 overlapping ingredients:\")\n",
    "    print(sorted(list(overlap))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge and Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both dataframes\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(f\"Combined shape before deduplication: {df_combined.shape}\")\n",
    "print(f\"Unique ingredients before deduplication: {df_combined['ingredient_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on ingredient_name (keep first occurrence)\n",
    "df_clean = df_combined.drop_duplicates(subset='ingredient_name', keep='first')\n",
    "\n",
    "print(f\"\\nCleaned shape after deduplication: {df_clean.shape}\")\n",
    "print(f\"Unique ingredients after deduplication: {df_clean['ingredient_name'].nunique()}\")\n",
    "print(f\"\\nRows removed: {df_combined.shape[0] - df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "# Display sample\n",
    "print(\"Sample of cleaned data:\")\n",
    "display(df_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing = df_clean.isnull().sum()\n",
    "missing_pct = (missing / len(df_clean)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "display(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty strings\n",
    "print(\"Empty string counts per column:\")\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == 'object':\n",
    "        empty_count = (df_clean[col] == '').sum()\n",
    "        if empty_count > 0:\n",
    "            print(f\"  {col}: {empty_count} ({empty_count/len(df_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rating from rating column (remove 'Rating: ' prefix)\n",
    "df_clean['rating_clean'] = df_clean['rating'].str.replace('Rating: ', '', regex=False)\n",
    "\n",
    "# Rating distribution\n",
    "print(\"Rating Distribution:\")\n",
    "rating_counts = df_clean['rating_clean'].value_counts()\n",
    "print(rating_counts)\n",
    "print(f\"\\nSample ratings to verify cleaning:\")\n",
    "print(df_clean[['ingredient_name', 'rating', 'rating_clean']].head(10))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "rating_order = ['Best', 'Good', 'Average', 'Bad', 'Worst']\n",
    "rating_counts = rating_counts.reindex([r for r in rating_order if r in rating_counts.index])\n",
    "rating_counts.plot(kind='bar', color=['#2ecc71', '#3498db', '#f39c12', '#e67e22', '#e74c3c'])\n",
    "plt.title('Distribution of Ingredient Ratings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Rating', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benefits analysis - convert to list format\n",
    "def extract_benefits(benefits_str):\n",
    "    \"\"\"Extract benefits from string and return as list\"\"\"\n",
    "    if pd.isna(benefits_str) or benefits_str == '':\n",
    "        return []\n",
    "    if isinstance(benefits_str, str) and benefits_str.startswith('Benefits:'):\n",
    "        benefits = benefits_str.replace('Benefits: ', '').split(', ')\n",
    "        return [b.strip() for b in benefits if b.strip()]\n",
    "    return []\n",
    "\n",
    "df_clean['benefits_list'] = df_clean['benefits'].apply(extract_benefits)\n",
    "\n",
    "has_benefits = df_clean['benefits_list'].apply(len) > 0\n",
    "print(\"Ingredients with benefits:\")\n",
    "print(f\"  Count: {has_benefits.sum()}\")\n",
    "print(f\"  Percentage: {has_benefits.sum()/len(df_clean)*100:.2f}%\")\n",
    "\n",
    "# Extract unique benefits\n",
    "all_benefits = [benefit for benefits_list in df_clean['benefits_list'] for benefit in benefits_list]\n",
    "\n",
    "if all_benefits:\n",
    "    benefits_series = pd.Series(all_benefits)\n",
    "    print(\"\\nMost common benefits:\")\n",
    "    print(benefits_series.value_counts().head(10))\n",
    "    \n",
    "print(\"\\nSample benefits data:\")\n",
    "display(df_clean[has_benefits][['ingredient_name', 'benefits', 'benefits_list']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories analysis - convert to list format\n",
    "def extract_categories(categories_str):\n",
    "    \"\"\"Extract categories from string and return as list\"\"\"\n",
    "    if pd.isna(categories_str) or categories_str == '':\n",
    "        return []\n",
    "    if isinstance(categories_str, str) and categories_str.startswith('Categories:'):\n",
    "        categories = categories_str.replace('Categories: ', '').split(', ')\n",
    "        return [c.strip() for c in categories if c.strip()]\n",
    "    return []\n",
    "\n",
    "df_clean['categories_list'] = df_clean['categories'].apply(extract_categories)\n",
    "\n",
    "has_categories = df_clean['categories_list'].apply(len) > 0\n",
    "print(\"Ingredients with categories:\")\n",
    "print(f\"  Count: {has_categories.sum()}\")\n",
    "print(f\"  Percentage: {has_categories.sum()/len(df_clean)*100:.2f}%\")\n",
    "\n",
    "# Extract unique categories\n",
    "all_categories = [category for categories_list in df_clean['categories_list'] for category in categories_list]\n",
    "\n",
    "if all_categories:\n",
    "    categories_series = pd.Series(all_categories)\n",
    "    print(\"\\nMost common categories:\")\n",
    "    top_categories = categories_series.value_counts().head(15)\n",
    "    print(top_categories)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_categories.plot(kind='barh', color='steelblue')\n",
    "    plt.title('Top 15 Ingredient Categories', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Count', fontsize=12)\n",
    "    plt.ylabel('Category', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"\\nSample categories data:\")\n",
    "display(df_clean[has_categories][['ingredient_name', 'categories', 'categories_list']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO_GLANCE analysis\n",
    "print(\"Ingredients with detailed info:\")\n",
    "has_info = df_clean['INFO_GLANCE'].notna() & (df_clean['INFO_GLANCE'] != '')\n",
    "print(f\"  Count: {has_info.sum()}\")\n",
    "print(f\"  Percentage: {has_info.sum()/len(df_clean)*100:.2f}%\")\n",
    "\n",
    "# Check for sentence separation issues\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING INFO_GLANCE FORMATTING ISSUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample ingredients with info to check formatting\n",
    "sample_info = df_clean[has_info][['ingredient_name', 'INFO_GLANCE']].head(10)\n",
    "\n",
    "print(\"\\nSample INFO_GLANCE entries (checking for sentence gluing):\")\n",
    "for idx, row in sample_info.iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Ingredient: {row['ingredient_name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    info_text = row['INFO_GLANCE']\n",
    "    # Check if there are lowercase letters immediately after uppercase (sign of glued sentences)\n",
    "    import re\n",
    "    potential_glue = re.findall(r'[a-z][A-Z]', info_text)\n",
    "    if potential_glue:\n",
    "        print(f\"‚ö†Ô∏è  POTENTIAL GLUED SENTENCES DETECTED: {potential_glue}\")\n",
    "    print(f\"Text: {info_text[:200]}...\" if len(info_text) > 200 else f\"Text: {info_text}\")\n",
    "    print()\n",
    "\n",
    "# Count how many have potential issues\n",
    "def check_glued_sentences(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return False\n",
    "    import re\n",
    "    return bool(re.search(r'[a-z][A-Z]', text))\n",
    "\n",
    "glued_count = df_clean[has_info]['INFO_GLANCE'].apply(check_glued_sentences).sum()\n",
    "print(f\"\\n‚ö†Ô∏è  Entries with potential glued sentences: {glued_count} out of {has_info.sum()} ({glued_count/has_info.sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fix INFO_GLANCE Sentence Separation\n",
    "\n",
    "The INFO_GLANCE data contains bullet points that were scraped without separators.\n",
    "Let's convert them into a proper list structure (like benefits and categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split glued sentences into list of bullet points\n",
    "def extract_info_bullets(text):\n",
    "    \"\"\"Split INFO_GLANCE text into list of bullet points\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return []\n",
    "    \n",
    "    import re\n",
    "    # Split on pattern: lowercase followed by uppercase (start of new sentence)\n",
    "    # This captures bullet point boundaries\n",
    "    sentences = re.split(r'(?<=[a-z])(?=[A-Z])', text)\n",
    "    \n",
    "    # Clean up each sentence\n",
    "    bullets = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return bullets\n",
    "\n",
    "df_clean['info_bullets'] = df_clean['INFO_GLANCE'].apply(extract_info_bullets)\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"Before and After - Converting to bullet point lists:\")\n",
    "print(\"=\"*80)\n",
    "sample_info = df_clean[df_clean['INFO_GLANCE'].notna() & (df_clean['INFO_GLANCE'] != '')].head(5)\n",
    "\n",
    "for idx, row in sample_info.iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Ingredient: {row['ingredient_name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nBEFORE (glued text):\")\n",
    "    print(f\"  {row['INFO_GLANCE'][:200]}...\" if len(row['INFO_GLANCE']) > 200 else f\"  {row['INFO_GLANCE']}\")\n",
    "    print(f\"\\nAFTER (bullet list):\")\n",
    "    for i, bullet in enumerate(row['info_bullets'], 1):\n",
    "        print(f\"  {i}. {bullet}\")\n",
    "\n",
    "# Statistics\n",
    "has_info = df_clean['info_bullets'].apply(len) > 0\n",
    "avg_bullets = df_clean[has_info]['info_bullets'].apply(len).mean()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä INFO_GLANCE Statistics:\")\n",
    "print(f\"  Ingredients with info: {has_info.sum()} ({has_info.sum()/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Average bullet points per ingredient: {avg_bullets:.1f}\")\n",
    "print(f\"  Data structure: List of strings (like benefits/categories)\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean Column Selection\n",
    "\n",
    "Let's select only the relevant columns for the SkinGen project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns including the cleaned list versions\n",
    "df_final = df_clean[[\n",
    "    'ingredient_name',\n",
    "    'rating_clean',\n",
    "    'benefits_list',\n",
    "    'categories_list',\n",
    "    'info_bullets'\n",
    "]].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "df_final.columns = ['ingredient_name', 'rating', 'benefits', 'categories', 'info']\n",
    "\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nColumn data types:\")\n",
    "print(df_final.dtypes)\n",
    "print(f\"\\nSample of final data:\")\n",
    "display(df_final.head())\n",
    "\n",
    "print(\"\\nüìã All columns are now in LIST format:\")\n",
    "sample_row = df_final[df_final['info'].apply(len) > 0].iloc[0]\n",
    "print(f\"\\nExample - {sample_row['ingredient_name']}:\")\n",
    "print(f\"  Rating: {sample_row['rating']}\")\n",
    "print(f\"  Benefits: {sample_row['benefits']}\")\n",
    "print(f\"  Categories: {sample_row['categories']}\")\n",
    "print(f\"  Info bullets:\")\n",
    "for i, bullet in enumerate(sample_row['info'], 1):\n",
    "    print(f\"    {i}. {bullet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checks\n",
    "print(\"Final Data Quality Checks:\")\n",
    "print(f\"1. Total ingredients: {len(df_final)}\")\n",
    "print(f\"2. Unique ingredients: {df_final['ingredient_name'].nunique()}\")\n",
    "print(f\"3. Duplicate ingredient names: {df_final['ingredient_name'].duplicated().sum()}\")\n",
    "print(f\"4. Missing ingredient names: {df_final['ingredient_name'].isnull().sum()}\")\n",
    "print(f\"5. Missing ratings: {df_final['rating'].isnull().sum()}\")\n",
    "\n",
    "# Check rating values\n",
    "print(\"\\n6. Valid rating values:\")\n",
    "print(df_final['rating'].unique())\n",
    "\n",
    "# Check list columns\n",
    "print(\"\\n7. Benefits list format check:\")\n",
    "print(f\"   Type: {type(df_final['benefits'].iloc[0])}\")\n",
    "sample_with_benefits = df_final[df_final['benefits'].apply(len) > 0].iloc[0]\n",
    "print(f\"   Sample: {sample_with_benefits['ingredient_name']} -> {sample_with_benefits['benefits']}\")\n",
    "\n",
    "print(\"\\n8. Categories list format check:\")\n",
    "print(f\"   Type: {type(df_final['categories'].iloc[0])}\")\n",
    "sample_with_cats = df_final[df_final['categories'].apply(len) > 0].iloc[0]\n",
    "print(f\"   Sample: {sample_with_cats['ingredient_name']} -> {sample_with_cats['categories']}\")\n",
    "\n",
    "print(\"\\n9. Info bullets list format check:\")\n",
    "print(f\"   Type: {type(df_final['info'].iloc[0])}\")\n",
    "sample_with_info = df_final[df_final['info'].apply(len) > 0].iloc[0]\n",
    "print(f\"   Sample: {sample_with_info['ingredient_name']}\")\n",
    "print(f\"   Bullets: {sample_with_info['info'][:3]}...\")  # Show first 3 bullets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet - preserves list structure natively!\n",
    "parquet_path = '/mnt/user-data/outputs/ingredients_cleaned.parquet'\n",
    "df_final.to_parquet(parquet_path, index=False, engine='pyarrow')\n",
    "print(f\"‚úÖ Cleaned data saved to: {parquet_path}\")\n",
    "print(\"   Parquet format preserves list structure natively!\")\n",
    "print(\"   Load with: pd.read_parquet('ingredients_cleaned.parquet')\")\n",
    "\n",
    "# Also save as CSV for compatibility (with JSON-encoded lists)\n",
    "import json\n",
    "df_to_save = df_final.copy()\n",
    "df_to_save['benefits'] = df_to_save['benefits'].apply(json.dumps)\n",
    "df_to_save['categories'] = df_to_save['categories'].apply(json.dumps)\n",
    "df_to_save['info'] = df_to_save['info'].apply(json.dumps)\n",
    "\n",
    "csv_path = '/mnt/user-data/outputs/ingredients_cleaned.csv'\n",
    "df_to_save.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Also saved as CSV (for compatibility): {csv_path}\")\n",
    "print(\"   Note: All list columns are JSON strings in CSV\")\n",
    "\n",
    "# Compare file sizes\n",
    "import os\n",
    "parquet_size = os.path.getsize(parquet_path) / 1024  # KB\n",
    "csv_size = os.path.getsize(csv_path) / 1024  # KB\n",
    "print(f\"\\nüìä File size comparison:\")\n",
    "print(f\"   Parquet: {parquet_size:.2f} KB\")\n",
    "print(f\"   CSV: {csv_size:.2f} KB\")\n",
    "print(f\"   Compression ratio: {csv_size/parquet_size:.2f}x\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original files combined: {df1.shape[0] + df2.shape[0]} rows\")\n",
    "print(f\"Duplicates removed: {(df1.shape[0] + df2.shape[0]) - len(df_final)}\")\n",
    "print(f\"Final clean dataset: {len(df_final)} unique ingredients\")\n",
    "print(f\"\\nData structure (ALL as lists):\")\n",
    "print(f\"  - ingredient_name: string\")\n",
    "print(f\"  - rating: string (Best/Good/Average/Bad/Worst)\")\n",
    "print(f\"  - benefits: list of strings\")\n",
    "print(f\"  - categories: list of strings\")\n",
    "print(f\"  - info: list of strings (bullet points)\")\n",
    "print(\"\\nüí° Recommendation: Use Parquet for your ML pipeline!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Analysis: Ingredient Quality by Category\n",
    "\n",
    "Let's analyze which categories tend to have better or worse ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rating score for analysis\n",
    "rating_score = {'Best': 5, 'Good': 4, 'Average': 3, 'Bad': 2, 'Worst': 1}\n",
    "df_final['rating_score'] = df_final['rating'].map(rating_score)\n",
    "\n",
    "# Explode categories to have one row per category (categories are already lists)\n",
    "df_exploded = df_final.explode('categories')\n",
    "df_exploded = df_exploded[df_exploded['categories'] != '']\n",
    "df_exploded = df_exploded[df_exploded['categories'].notna()]\n",
    "\n",
    "# Calculate average rating per category\n",
    "category_ratings = df_exploded.groupby('categories')['rating_score'].agg(['mean', 'count']).reset_index()\n",
    "category_ratings.columns = ['Category', 'Avg_Rating', 'Count']\n",
    "category_ratings = category_ratings[category_ratings['Count'] >= 10]  # Filter for categories with at least 10 ingredients\n",
    "category_ratings = category_ratings.sort_values('Avg_Rating', ascending=False)\n",
    "\n",
    "print(\"Top 10 highest-rated categories (min 10 ingredients):\")\n",
    "display(category_ratings.head(10))\n",
    "\n",
    "print(\"\\nTop 10 lowest-rated categories (min 10 ingredients):\")\n",
    "display(category_ratings.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top and bottom categories\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top categories\n",
    "top_10 = category_ratings.head(10)\n",
    "ax1.barh(top_10['Category'], top_10['Avg_Rating'], color='green', alpha=0.7)\n",
    "ax1.set_xlabel('Average Rating Score', fontsize=12)\n",
    "ax1.set_title('Top 10 Highest-Rated Categories', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 5)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Bottom categories\n",
    "bottom_10 = category_ratings.tail(10)\n",
    "ax2.barh(bottom_10['Category'], bottom_10['Avg_Rating'], color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Average Rating Score', fontsize=12)\n",
    "ax2.set_title('Top 10 Lowest-Rated Categories', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0, 5)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The ingredient data has been successfully cleaned and prepared for the SkinGen project. Key findings:\n",
    "\n",
    "- Started with 2 files containing potentially overlapping data\n",
    "- Removed duplicates to create a clean dataset\n",
    "- Analyzed ingredient ratings, benefits, and categories\n",
    "- Identified high-quality and potentially problematic ingredient categories\n",
    "\n",
    "**Next Steps:**\n",
    "1. Use this cleaned ingredient database for product analysis\n",
    "2. Develop the ingredient intelligence layer for SkinGen\n",
    "3. Integrate with product recommendations system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
